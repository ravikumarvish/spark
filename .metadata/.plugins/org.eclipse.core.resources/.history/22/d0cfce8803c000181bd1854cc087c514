/**
 * 
 */
package com.livefortech.spark.rdd;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import ch.qos.logback.classic.Level;

/**
 * @author ravikumarvish
 *
 */
public class EmployeeTransformation {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
		Logger.getLogger("org").setLevel(Level.ERROR);
		
		SparkConf conf = new SparkConf().setAppName("Spark Transformation App").setMaster("local[1]");
		JavaSparkContext context = new JavaSparkContext(conf);
		
		JavaRDD<String> employees = context.textFile("in/employee.txt");
		JavaRDD<String> usEmployees = employees.filter(employee -> { 
			return employee.split("\t")[2].equals("US");
		});
		
		System.out.println("***********USEmployee"+usEmployees.count());
		
		JavaRDD<String> emplyeeNameWithAge  = usEmployees.map(employee -> {
			String[] records = employee.split("\t");
			return records[1] + "\t" + records[3];
		});
		
		
		emplyeeNameWithAge.saveAsTextFile("out/USEmployee");
		
	}

}
